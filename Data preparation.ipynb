{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e6d389e96998f597cee0fc4b39534997436beb1ddafc62a05299fd5c50d2177c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this notebook we do data preparation. This entails to augmenting the data by performing 10 random elastic deformations on each of the 30 images and masks producing a total of 300 training images and masks.\n",
    "\n",
    "We start by installing the necessary packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-06-23 18:53:31.464 | WARNING  | torch_snippets.torch_loader:<module>:233 - Not importing Lightning Report\n",
      "2021-06-23 18:53:36.224 | WARNING  | torch_snippets:<module>:13 - sklearn is not found. Skipping relevant imports from submodule `sklegos`\n",
      "Exception: No module named 'sklego'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# stems(...) belongs to torch_snippets\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import PurePath\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import matplotlib\n",
    "from tqdm import tnrange,notebook\n",
    "import cv2 as cv2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "x(::-1) reverses the top level numpy array (horizontal mirroring of image). Vertical mirroring is done by flip(...). Rotation by 180 degrees is achieved by combining vertical and horizontal mirroring in any order."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirrorExtrapolation(x):\n",
    "    VertMir=np.flip(x,1)\n",
    "    rot180=np.flip(x[::-1],1)\n",
    "    topBottom=np.hstack((rot180,x[::-1],rot180))\n",
    "    return np.vstack((topBottom,np.hstack((VertMir,x,VertMir)),topBottom))"
   ]
  },
  {
   "source": [
    "And now for the random elastic deformations. The function generateRandomElastic2D(...) generas a 572*572 array of random displacements following the outline of the paper. The implementation is slow, so we should apply it only once during the data preparation stage and not during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomElastic1D():\n",
    "    displacements=np.random.normal(loc=0.0, scale=10.0, size=(4,4))\n",
    "    x = np.array([0.0, 1.0, 2.0, 3.0])\n",
    "    y = np.array([0.0, 1.0, 2.0, 3.0])\n",
    "    f = interpolate.interp2d(x, y, displacements, kind='cubic')\n",
    "    gridDisplacements = np.empty([572, 572], dtype=int)\n",
    "   \n",
    "    for x in range(572):\n",
    "        for y in range(572):\n",
    "            gridDisplacements[x][y] = int(f(3.0 * x / 571.0,3.0 * y / 571.0))\n",
    "        \n",
    "    return gridDisplacements\n",
    "\n",
    "def generateRandomElastic2D():\n",
    "    gridDisplacementsx = generateRandomElastic1D()\n",
    "    gridDisplacementsy = generateRandomElastic1D()\n",
    "    gridDisplacements=np.array([gridDisplacementsx,gridDisplacementsy])\n",
    "    gridDisplacements = gridDisplacements.swapaxes(0,2)\n",
    "    return gridDisplacements"
   ]
  },
  {
   "source": [
    "Vi now execute the elastic deformations on our image base. The method being to use the displacements in a backward manner in the middle (572*572) size portion of our mirror extrapolated image. For each pixel in this middle region we calculate what pixel in the larger image should be copied to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Given a grid displacement array and an image we now calculate the deformed image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elasticDeformation(img, displacement):\n",
    "    extrapolated = mirrorExtrapolation(img)\n",
    "    deformed = np.empty_like(img)\n",
    "\n",
    "    for x in range(572):\n",
    "        for y in range(572):\n",
    "            deformed[x][y]=extrapolated[x + 572 + displacement[x][y][0]][y + 572 + displacement[x][y][1]]\n",
    "\n",
    "    return deformed"
   ]
  },
  {
   "source": [
    "The generation of elastic deformations was the heavy duty job above, so we would like to create a bunch of them and save to disk, to be reused later. The method of generating a new batch of 10 displacements will be to just delete the deformations folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NofDeformations = 10\n",
    "if not os.path.exists('isbi-datasets-master\\deformations'):\n",
    "    os.mkdir(\"isbi-datasets-master\\deformations\")\n",
    "    gridDisplacements = np.empty([NofDeformations, 572, 572, 2], dtype=int)\n",
    "    with open('isbi-datasets-master\\deformations\\deformations.npy', 'wb') as f:\n",
    "        for x in range(NofDeformations):\n",
    "            print(x)\n",
    "            gridDisplacements[x] = generateRandomElastic2D()\n",
    "            np.save(f, gridDisplacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m572\u001b[0m, \u001b[1;36m572\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">572</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">572</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n</pre>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "if os.path.exists('isbi-datasets-master\\deformations'):\n",
    "    with open('isbi-datasets-master\\deformations\\deformations.npy', 'rb') as f:\n",
    "        gridDisplacements = np.load(f)\n",
    "\n",
    "    print(gridDisplacements.shape)"
   ]
  },
  {
   "source": [
    "Now that we have 10 displacements we apply each to our 30 images and create a folder for that purpose. We need a Dataset instance to iterate the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegData(Dataset):\n",
    "    def __init__(self, aug=None):\n",
    "        self.items = stems(f'isbi-datasets-master/data/images')\n",
    "        self.aug = aug\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, ix):\n",
    "        name = PurePath(self.items[ix]).name[-2:]\n",
    "        image = read(f'isbi-datasets-master/data/images/train-volume{name}.jpg',1)\n",
    "        image = cv2.resize(image, (572,572))\n",
    "        mask = read(f'isbi-datasets-master/data/labels/train-labels{name}.jpg')\n",
    "        mask = cv2.resize(mask, (572,572))\n",
    "        return image, mask\n",
    "    def choose(self): return self[randint(len(self))]\n",
    "    def getStem(self,ix): return PurePath(self.items[ix]).name[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-06-23 18:54:28.930 | INFO     | torch_snippets.loader:stems:63 - 30 files found at isbi-datasets-master/data/images\n"
     ]
    }
   ],
   "source": [
    "ds = SegData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('isbi-datasets-master\\data\\deformed_images'):\n",
    "    os.mkdir(\"isbi-datasets-master\\data\\deformed_images\")\n",
    "    for j in notebook.tnrange(len(gridDisplacements), desc='deformed_images'):\n",
    "        for i in range(len(ds)):\n",
    "            deformed = elasticDeformation(ds[i][0],gridDisplacements[j])\n",
    "            write(deformed,f'isbi-datasets-master/data/deformed_images/train-volume{ds.getStem(i)}_{j:02d}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('isbi-datasets-master\\data\\deformed_labels'):\n",
    "    os.mkdir(\"isbi-datasets-master\\data\\deformed_labels\")\n",
    "    for j in notebook.tnrange(len(gridDisplacements), desc='deformed_labels'):\n",
    "        for i in range(len(ds)):\n",
    "            deformed = elasticDeformation(ds[i][1],gridDisplacements[j])\n",
    "            write(deformed,f'isbi-datasets-master/data/deformed_labels/train-labels{ds.getStem(i)}_{j:02d}.jpg')"
   ]
  },
  {
   "source": [
    "Now that we have deformed our labels, we would like to convert these gray scale images into binary images, where 0 means backround and 1 means a cell border pixel.\n",
    "\n",
    "We start out with a Dataset, to easily iterate through the deformed labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformedLabels(Dataset):\n",
    "    def __init__(self):\n",
    "        self.items = stems(f'isbi-datasets-master/data/deformed_labels')\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, ix):\n",
    "        name = PurePath(self.items[ix]).name[-5:]\n",
    "        mask = read(f'isbi-datasets-master/data/deformed_labels/train-labels{name}.jpg')\n",
    "        return mask\n",
    "    def choose(self): return self[randint(len(self))]\n",
    "    def getStem(self,ix): return PurePath(self.items[ix]).name[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-06-23 18:54:41.521 | INFO     | torch_snippets.loader:stems:63 - 300 files found at isbi-datasets-master/data/deformed_labels\n"
     ]
    }
   ],
   "source": [
    "labels = DeformedLabels()"
   ]
  },
  {
   "source": [
    "Now that we have our data, we create a new folder for our binary images and convert from gray scale to binary using bimodal histogram thresholding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('isbi-datasets-master\\data\\deformed_binary_labels'):\n",
    "    os.mkdir(\"isbi-datasets-master\\data\\deformed_binary_labels\")\n",
    "    for i in notebook.tnrange(len(labels), desc='deformed_binary_labels'):\n",
    "        mask = labels[i]\n",
    "        ret,th = cv2.threshold(mask,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # Otsu's thresholding\n",
    "        name = labels.getStem(i)\n",
    "        write(th,f'isbi-datasets-master/data/deformed_binary_labels/train-labels{name}.png')"
   ]
  },
  {
   "source": [
    "Proof of concept of above method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = labels[30]\n",
    "ret,th = cv2.threshold(mask,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # Otsu's thresholding\n",
    "matplotlib.pyplot.hist(th.flatten(), bins='auto')\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "source": [
    "Given the binary files we see that they are somewhat imbalanced and we compute the weight maps described in the paper."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "deformed_binary_labels:   0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60d99abc6eac42f38bc7f3663ef9aeba"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "if os.path.exists('isbi-datasets-master\\data\\deformed_binary_labels') and not os.path.exists('isbi-datasets-master\\data\\wc.npy'):\n",
    "    wc0=np.zeros((572, 572))\n",
    "    wc1=np.full((572, 572), NofDeformations * 30)\n",
    "    for i in notebook.tnrange(len(labels), desc='deformed_binary_labels'):\n",
    "        name = labels.getStem(i)\n",
    "        mask = read(f'isbi-datasets-master/data/deformed_binary_labels/train-labels{name}.png')\n",
    "        mask = mask/255\n",
    "        wc0 = wc0 + mask\n",
    "\n",
    "    wc1 = wc1 - wc0\n",
    "    wc0 = wc0/(NofDeformations * 30)\n",
    "    wc1 = wc1/(NofDeformations * 30)\n",
    "\n",
    "    wc0 = np.expand_dims(wc0, axis=2)\n",
    "    wc1 = np.expand_dims(wc1, axis=2)\n",
    "    wc  = np.concatenate((wc0, wc1), axis=2)\n",
    "    #print(wc)\n",
    "\n",
    "    with open('isbi-datasets-master\\data\\wc.npy', 'wb') as f:\n",
    "        np.save(f, wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('isbi-datasets-master\\data\\wc.npy', 'rb') as f:\n",
    "    wc = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.shape"
   ]
  },
  {
   "source": [
    "And now for the two maps d1 and d2 have to be computed per image. The thing is quite complicated, but can be implemented using the following steps:\n",
    "\n",
    "a) Calculate the connected foreground components of the binary images\n",
    "b) For each pair of components calculate the min distance between them\n",
    "c) For each component find the nearest and second nearest component\n",
    "d) For each foreground pixel calculate the distance to the nearest and second nearest component\n",
    "\n",
    "The computations of connected components is risky as there might be \"holes\" in the borders. This could be fixed by the mentioned morphological operations, but a much simpler approech for the ISBI2012 dataset is to simply calculate the distance of each pixel to the nearest neighbour (d1) and ignoring the second nearest neighbour (d2). This can then be achieved by the distance transform, which we calculate for each mask and save in a folder.\n",
    "\n",
    "In order to simplify both the later calculations of the loss function and its debugging friendliness we simply precompute w(x) for each training image and save as a tensor\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "wx:   0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be75ae6c79cb49db8b949ea59df3b1b6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "if not os.path.exists('isbi-datasets-master\\data\\wx'):\n",
    "    w0  = 10\n",
    "    sigma = 5\n",
    "    os.mkdir(\"isbi-datasets-master\\data\\wx\")\n",
    "    for i in notebook.tnrange(len(labels), desc='wx'):\n",
    "        name = labels.getStem(i)\n",
    "        mask = read(f'isbi-datasets-master/data/deformed_binary_labels/train-labels{name}.png')\n",
    "        d = torch.from_numpy(cv2.distanceTransform(mask,cv2.DIST_L2,3))\n",
    "        wx = w0 * torch.exp(-torch.mul(d, d) / (sigma * sigma))\n",
    "        with open('isbi-datasets-master/data/wx/wx' + name + \".npy\", 'wb') as f:\n",
    "            np.save(f, wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('isbi-datasets-master\\data\\distance_transforms'):\n",
    "    os.mkdir(\"isbi-datasets-master\\data\\distance_transforms\")\n",
    "    for i in notebook.tnrange(len(labels), desc='distance_transforms'):\n",
    "        name = labels.getStem(i)\n",
    "        mask = read(f'isbi-datasets-master/data/deformed_binary_labels/train-labels{name}.png')\n",
    "        dist_transform = cv2.distanceTransform(mask,cv2.DIST_L2,3)\n",
    "        with open('isbi-datasets-master/data/distance_transforms/distance-transform' + name + \".npy\", 'wb') as f:\n",
    "            np.save(f, dist_transform)"
   ]
  },
  {
   "source": [
    "Lets check that the distance transforms stored look ok. We know that there are 71455 zero pixels in the mask image below (00_00). This is consistent with the distance transform histogram showing that amount of pixels having zero distance to a border. The zero pixels are the borders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('isbi-datasets-master/data/distance_transforms/distance-transform00_00.npy', 'rb') as f:\n",
    "    dist_transform = np.load(f)\n",
    "\n",
    "mask = read(f'isbi-datasets-master/data/deformed_binary_labels/train-labels00_00.png')\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "matplotlib.pyplot.hist(mask.flatten(), bins='auto')\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "plt.imshow(dist_transform)\n",
    "plt.show()\n",
    "\n",
    "print(\"NUmber of zero pixels: \" + str(mask[np.where(mask == 0)].size))\n",
    "matplotlib.pyplot.hist(dist_transform.flatten(), bins='auto')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}