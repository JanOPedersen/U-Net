{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e6d389e96998f597cee0fc4b39534997436beb1ddafc62a05299fd5c50d2177c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we attempt to reproduce the results from the paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". We only focus on the ISBI 2012 dataset, as this was the only one we could get our hands on.\r\n",
    "\r\n",
    "We start by installing the necessary packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by installing the necessary packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch_snippets import Dataset, stems, read, cv2, randint, DataLoader, nn, torchvision, optim, Report\r\n",
    "from torchvision import transforms\r\n",
    "from pathlib import PurePath\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "import torch \r\n",
    "from torchsummary import summary\r\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tfms = transforms.Compose([\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the Dataset class with its built in augmentation mechanism"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SegData(Dataset):\r\n",
    "    def __init__(self, aug=None):\r\n",
    "        self.items = stems('isbi-datasets-master/data/deformed_images')\r\n",
    "        self.aug = aug\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.items)\r\n",
    "    def __getitem__(self, ix):\r\n",
    "        name = PurePath(self.items[ix]).name[-5:]\r\n",
    "        image = read(f'isbi-datasets-master/data/deformed_images/train-volume{name}.jpg',1)\r\n",
    "        image = cv2.resize(image, (572,572))\r\n",
    "        mask = read(f'isbi-datasets-master/data/deformed_binary_labels/train-labels{name}.png')\r\n",
    "        mask = cv2.resize(mask, (572,572))\r\n",
    "        with open('isbi-datasets-master/data/wx/wx' + name + '.npy', 'rb') as file:\r\n",
    "            wx = np.load(file)\r\n",
    "        return image, mask, wx\r\n",
    "    def choose(self): return self[randint(len(self))]\r\n",
    "    def collate_fn(self, batch):\r\n",
    "        ims, masks, wxs = list(zip(*batch))\r\n",
    "        ims = torch.cat([tfms(im.copy()/255.)[None] for im in ims]).float().to(DEVICE)\r\n",
    "        ce_masks = torch.cat([torch.Tensor(mask[None]) for mask in masks]).long().to(DEVICE)\r\n",
    "        wxs = torch.cat([torch.Tensor(wx[None]) for wx in wxs]).float().to(DEVICE)\r\n",
    "        return ims, ce_masks, wxs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the dataset instance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trn_ds = SegData()\r\n",
    "trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True, collate_fn=trn_ds.collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the down and up convolutional blocks\n",
    "\n",
    "More importantly, however, unless you can explicitly justify it, I advise against using BatchNormalization with batch_size=1; there are strong theoretical reasons against it, and multiple publications have shown BN performance degrade for batch_size under 32, and severely for <=8. In a nutshell, batch statistics \"averaged\" over a single sample vary greatly sample-to-sample (high variance), and BN mechanisms don't work as intended."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def conv(in_channels, out_channels):\r\n",
    "    return nn.Sequential(\r\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=0),\r\n",
    "        nn.ReLU(inplace=True)\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def up_conv(in_channels, out_channels):\r\n",
    "    return nn.Sequential(\r\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\r\n",
    "        nn.ReLU(inplace=True)\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the fun starts when we define the network architecture. The notation convXY means layer X and conv number Y in that layer. Where a layer is the level in the U-Net drawing figure 1 in the paper.\n",
    "\n",
    "Why does the nn.init.normal_(...) function divide the fan_out with 2 before generating the Gaussian distributed real numbers? Actually in the paper the quantity sqrt(2/N) is mentioned:\n",
    "\n",
    "It can be seen on the histogram later that removing the self._init_weights() below results in uniformly random numbers and not Gaussian."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UNet(nn.Module):\r\n",
    "    def __init__(self, out_channels=2):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.conv11 = conv(3, 64)\r\n",
    "        self.conv12 = conv(64, 64)\r\n",
    "\r\n",
    "        self.conv21 = conv(64, 128)\r\n",
    "        self.conv22 = conv(128, 128)\r\n",
    "\r\n",
    "        self.conv31 = conv(128, 256)\r\n",
    "        self.conv32 = conv(256, 256)\r\n",
    "\r\n",
    "        self.conv41 = conv(256, 512)\r\n",
    "        self.conv42 = conv(512, 512)\r\n",
    "\r\n",
    "        self.conv51 = conv(512, 1024)\r\n",
    "        self.conv52 = conv(1024, 1024)\r\n",
    "\r\n",
    "        self.up_conv5 = up_conv(1024, 512)\r\n",
    "        self.conv43 = conv(512 + 512, 512)\r\n",
    "        self.conv44 = conv(512, 512)\r\n",
    "\r\n",
    "        self.up_conv4 = up_conv(512, 256)\r\n",
    "        self.conv33 = conv(256 + 256, 256)\r\n",
    "        self.conv34 = conv(256, 256)\r\n",
    "\r\n",
    "        self.up_conv3 = up_conv(256, 128)\r\n",
    "        self.conv23 = conv(128 + 128, 128)\r\n",
    "        self.conv24 = conv(128, 128)\r\n",
    "\r\n",
    "        self.up_conv2 = up_conv(128, 64)\r\n",
    "        self.conv13 = conv(64 + 64, 64)\r\n",
    "        self.conv14 = conv(64, 64)\r\n",
    "        self.conv15 = nn.Conv2d(64, out_channels, kernel_size=1)\r\n",
    "\r\n",
    "        self.maxPool = nn.MaxPool2d(2)\r\n",
    "\r\n",
    "        self.crop1 = torchvision.transforms.CenterCrop((392,392))\r\n",
    "        self.crop2 = torchvision.transforms.CenterCrop((200,200))\r\n",
    "        self.crop3 = torchvision.transforms.CenterCrop((104,104))\r\n",
    "        self.crop4 = torchvision.transforms.CenterCrop((56,56))\r\n",
    "\r\n",
    "        self.ReLU = nn.ReLU(inplace=True)\r\n",
    "\r\n",
    "        self._init_weights()\r\n",
    "\r\n",
    "    def _init_weights(self):\r\n",
    "        for m in self.modules():\r\n",
    "            if type(m) in {\r\n",
    "                nn.Linear,\r\n",
    "                nn.Conv2d,\r\n",
    "                nn.ConvTranspose2d\r\n",
    "            }:\r\n",
    "                _, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data)\r\n",
    "                std = 1 / math.sqrt(fan_out / 2)\r\n",
    "                nn.init.normal_(m.weight.data, 0, std)\r\n",
    "                if m.bias is not None:\r\n",
    "                    nn.init.normal_(m.weight.data, 0, std)\r\n",
    "\r\n",
    "    def forward(self, x):               # 572*572\r\n",
    "\r\n",
    "        x = self.conv11(x)              # 570*570\r\n",
    "        block1 = self.conv12(x)         # 568*568\r\n",
    "        x = self.maxPool(block1)        # 284*284\r\n",
    "       \r\n",
    "        x = self.conv21(x)              # 282*282\r\n",
    "\r\n",
    "        block2 = self.conv22(x)         # 280*280\r\n",
    "        x = self.maxPool(block2)        # 140*140\r\n",
    "       \r\n",
    "        x = self.conv31(x)              # 138 * 138\r\n",
    "        block3 = self.conv32(x)         # 136 * 136\r\n",
    "        x = self.maxPool(block3)        # 68 * 68\r\n",
    "\r\n",
    "        x = self.conv41(x)              # 66*66\r\n",
    "        block4 = self.conv42(x)         # 64*64 \r\n",
    "        x = self.maxPool(block4)        # 32*32\r\n",
    "\r\n",
    "        x = self.conv51(x)              # 30*30\r\n",
    "        x = self.conv52(x)              # 28*28\r\n",
    "        x = self.up_conv5(x)            # 56*56\r\n",
    "\r\n",
    "        block4 = self.crop4(block4)     # 56*56\r\n",
    "        x = torch.cat([x, block4], dim=1) \r\n",
    "        x = self.conv43(x)              # 54*54\r\n",
    "        x = self.conv44(x)              # 52*52\r\n",
    "        x = self.up_conv4(x)            # 104*104\r\n",
    "\r\n",
    "        block3 = self.crop3(block3)     # 104*104\r\n",
    "        x = torch.cat([x, block3], dim=1) \r\n",
    "        x = self.conv33(x)              # 102*102\r\n",
    "        x = self.conv34(x)              # 100*100\r\n",
    "        x = self.up_conv3(x)            # 200*200\r\n",
    "\r\n",
    "        block2 = self.crop2(block2)     # 200*200\r\n",
    "        x = torch.cat([x, block2], dim=1) \r\n",
    "        x = self.conv23(x)              # 198*198\r\n",
    "        x = self.conv24(x)              # 196*196\r\n",
    "        x = self.up_conv2(x)            # 392*392\r\n",
    "        \r\n",
    "        block1 = self.crop1(block1)     # 392*392\r\n",
    "        x = torch.cat([x, block1], dim=1) \r\n",
    "        x = self.conv13(x)              # 390*390\r\n",
    "        x = self.conv14(x)              # 388*388\r\n",
    "        x = self.conv15(x)              # 388*388\r\n",
    "        x = self.ReLU(x)                # 388*388\r\n",
    "        \r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we would like to know the input and mask tensors a bit better so we can appreciate the data processing that the model does"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MODEL = UNet().to(DEVICE)\r\n",
    "summary(MODEL, (3,572,572));"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now venture into defining the loss function from the paper. Here p is the predicted value (ground truth) and y is the NN output. d is the distance transform that belongs to the input image and that was defined in the data preparation stage.\n",
    "\n",
    "First we read the wc map from file and center crop it to 388*388 size to fit the NN output. The cropping could have been done in the data preparation stage. We also init the hyper parameters w0 and sigma that are used to define the loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(r'isbi-datasets-master\\data\\wc.npy', 'rb') as f:\r\n",
    "    wc  = torch.from_numpy(np.load(f)).to(DEVICE)\r\n",
    "    wc0 = torchvision.transforms.CenterCrop((388,388))(wc[:,:,0])\r\n",
    "    wc1 = torchvision.transforms.CenterCrop((388,388))(wc[:,:,1])\r\n",
    "    wc = torch.cat((wc0.unsqueeze(0),wc1.unsqueeze(0)),0).unsqueeze(0)\r\n",
    "\r\n",
    "W0    = 10\r\n",
    "SIGMA = 5\r\n",
    "BETA  = 1 # exp(base) of exponential in softmax function\r\n",
    "crop  = torchvision.transforms.CenterCrop((388,388))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def UnetLoss(outputs, targets, weights):\r\n",
    "    num_examples = targets.shape[1] * targets.shape[2]\r\n",
    "    targets = torch.unsqueeze(targets,1)\r\n",
    "    outputs = outputs - torch.logsumexp(outputs,dim=1, keepdim=True)\r\n",
    "    outputs = torch.gather(outputs,1,targets)[0,0,:,:]\r\n",
    "    weights = weights + torch.gather(wc,2,targets)[0,0,:,:]\r\n",
    "    outputs = outputs * weights\r\n",
    "    return - torch.sum(outputs)/num_examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_cross_entropy_loss(outputs, targets):\r\n",
    "    num_examples = targets.shape[1] * targets.shape[2]\r\n",
    "    targets = torch.unsqueeze(targets,1)\r\n",
    "    outputs = outputs - torch.logsumexp(outputs,dim=1, keepdim=True)\r\n",
    "    outputs = torch.gather(outputs,1,targets)[0,0,:,:]\r\n",
    "    return - torch.sum(outputs)/num_examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before entering the training loop let us do some sanity tests on the loss function. On an output that matches the mask we would like minimum loss, on an output that is the negation (255 - y) of the mask we would like maximum loss. Anything in between should give loss between these values.\n",
    "\n",
    "In summary we get minimum loss 0 and maximum loss ~ 1258570. The actual loss for the given input is ~ 629104. Which is consistent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before the training loop we define the what is inside it:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_batch(model, data, optimizer, criterion):\r\n",
    "    model.train()\r\n",
    "    im, mask, weights = data\r\n",
    "    out  = model(im)\r\n",
    "    mask = crop(mask)\r\n",
    "    mask = torch.unsqueeze(torch.where(mask[0,:,:] == 0, 0, 1), 0)\r\n",
    "    weights = crop(weights[0])\r\n",
    "    optimizer.zero_grad()\r\n",
    "    loss = criterion(out, mask, weights)\r\n",
    "    loss.backward()\r\n",
    "    optimizer.step()\r\n",
    "    return loss.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = UNet().to(DEVICE)\r\n",
    "criterion = UnetLoss\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\r\n",
    "n_epochs = 50\r\n",
    "log = Report(n_epochs)\r\n",
    "train_losses = []\r\n",
    "\r\n",
    "for ex in range(n_epochs):\r\n",
    "    N = len(trn_dl)\r\n",
    "    train_epoch_losses = []\r\n",
    "    for bx, data in enumerate(trn_dl):\r\n",
    "        loss = train_batch(model, data, optimizer, criterion)\r\n",
    "        train_epoch_losses.append(loss) \r\n",
    "        log.record(ex + (bx+1)/N, trn_loss=loss, trn_acc=1.0, end='\\r')\r\n",
    "    train_epoch_loss = np.array(train_epoch_losses).mean()\r\n",
    "\r\n",
    "    train_losses.append(train_epoch_loss)\r\n",
    "    log.report_avgs(ex+1)\r\n",
    "\r\n",
    "print('Training complete')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "log.plot_epochs(['trn_loss','val_loss'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('train_epoch_losses_20_epoch_weighted_CE_loss.npy', 'wb') as f:\r\n",
    "    np.save(f, train_losses)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the losses so that we can display the,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('train_epoch_losses_20_epoch_weighted_CE_loss.npy', 'rb') as f:\r\n",
    "    train_losses = np.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = np.arange(20)+1\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.ticker as mticker\r\n",
    "%matplotlib inline\r\n",
    "plt.subplot(211)\r\n",
    "plt.plot(epochs, train_losses, 'bo', label='Training loss')\r\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\r\n",
    "plt.title('Training loss with 0.01 learning rate')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Loss')\r\n",
    "plt.legend()\r\n",
    "plt.grid('off')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model, '20_epoch_weighted_CE_loss.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "im, Mask, dist_transform = next(iter(trn_dl))\r\n",
    "out = model(im)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets display, in order: The input image, the mask (ground truth) and the output image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mask0 = torchvision.transforms.CenterCrop((388,388))(Mask)\r\n",
    "mask0 = torch.moveaxis(mask0, 0, 2)\r\n",
    "\r\n",
    "im = crop(im)\r\n",
    "plt.imshow(im[0].permute(1,2,0).detach().cpu()[:,:,0])\r\n",
    "plt.show()\r\n",
    "mask0 = torch.cat((mask0, mask0, mask0), 2)\r\n",
    "plt.imshow(mask0.detach().cpu())\r\n",
    "plt.show()\r\n",
    "\r\n",
    "out1=out[0,:,:,:]\r\n",
    "white = torch.full((1, 388, 388), 255,device=DEVICE)\r\n",
    "out1 = torch.cat((white, out1),0)\r\n",
    "out1 = out1.permute(1,2,0)\r\n",
    "plt.imshow(out1.detach().cpu())\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}